{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e885ad25-d7a7-4b1e-9311-49e729b8e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run windows by kph (lists of [start, end]) ===\n",
      "20kph = [[1757914226.6105194, 1757917563.0991616], [1757914226.6105194, 1757914420.6993194], [1757914226.6105194, 1757914550.887972], [1757914226.6105194, 1757914667.5927155], [1757914226.6105194, 1757914786.275123], [1757914226.6105194, 1757914923.5330007], [1757914226.6105194, 1757916150.7172039], [1757917557.797726, 1757917675.8044224], [1757917557.797726, 1757917787.7237718], [1757917557.797726, 1757917900.7781775], [1757917557.797726, 1757918013.5039613]]\n",
      "30kph = [[1757914226.6105194, 1757915024.5590355], [1757914226.6105194, 1757915121.9400544], [1757914226.6105194, 1757915234.0027125], [1757914226.6105194, 1757915329.2626548], [1757914226.6105194, 1757915425.3140006], [1757917557.797726, 1757918125.5372443], [1757917557.797726, 1757918237.9573307], [1757917557.797726, 1757918350.3041887], [1757917557.797726, 1757918433.1740332], [1757917557.797726, 1757918531.0669913]]\n",
      "40kph = [[1757914226.6105194, 1757915528.026441], [1757914226.6105194, 1757915634.6433153], [1757914226.6105194, 1757915737.2484002], [1757914226.6105194, 1757915832.3079796], [1757914226.6105194, 1757915938.87873], [1757917557.797726, 1757918619.5116527], [1757917557.797726, 1757918733.4559057], [1757917557.797726, 1757918833.7639494], [1757917557.797726, 1757918923.8949573], [1757917557.797726, 1757919030.9990425]]\n",
      "50kph = [[1757914226.6105194, 1757916031.8637962], [1757917557.797726, 1757919112.457446], [1757917557.797726, 1757919220.2374144]]\n",
      "\n",
      "=== Run windows by kph (with filenames) ===\n",
      "20kph:\n",
      "  - 2509515_001_20kph+vir.txt: [1757914226.6105194, 1757917563.0991616]\n",
      "  - 2509515_001_20kph.txt: [1757914226.6105194, 1757914420.6993194]\n",
      "  - 2509515_002_20kph.txt: [1757914226.6105194, 1757914550.887972]\n",
      "  - 2509515_003_20kph.txt: [1757914226.6105194, 1757914667.5927155]\n",
      "  - 2509515_004_20kph.txt: [1757914226.6105194, 1757914786.275123]\n",
      "  - 2509515_005_20kph.txt: [1757914226.6105194, 1757914923.5330007]\n",
      "  - 2509515_006_20kph.txt: [1757914226.6105194, 1757916150.7172039]\n",
      "  - 2509515_002_20kph+vir.txt: [1757917557.797726, 1757917675.8044224]\n",
      "  - 2509515_003_20kph+vir.txt: [1757917557.797726, 1757917787.7237718]\n",
      "  - 2509515_004_20kph+vir.txt: [1757917557.797726, 1757917900.7781775]\n",
      "  - 2509515_005_20kph+vir.txt: [1757917557.797726, 1757918013.5039613]\n",
      "30kph:\n",
      "  - 2509515_001_30kph.txt: [1757914226.6105194, 1757915024.5590355]\n",
      "  - 2509515_002_30kph.txt: [1757914226.6105194, 1757915121.9400544]\n",
      "  - 2509515_003_30kph.txt: [1757914226.6105194, 1757915234.0027125]\n",
      "  - 2509515_004_30kph.txt: [1757914226.6105194, 1757915329.2626548]\n",
      "  - 2509515_005_30kph.txt: [1757914226.6105194, 1757915425.3140006]\n",
      "  - 2509515_001_30kph+vir.txt: [1757917557.797726, 1757918125.5372443]\n",
      "  - 2509515_002_30kph+vir.txt: [1757917557.797726, 1757918237.9573307]\n",
      "  - 2509515_003_30kph+vir.txt: [1757917557.797726, 1757918350.3041887]\n",
      "  - 2509515_004_30kph+vir.txt: [1757917557.797726, 1757918433.1740332]\n",
      "  - 2509515_005_30kph+vir.txt: [1757917557.797726, 1757918531.0669913]\n",
      "40kph:\n",
      "  - 2509515_001_40kph.txt: [1757914226.6105194, 1757915528.026441]\n",
      "  - 2509515_002_40kph.txt: [1757914226.6105194, 1757915634.6433153]\n",
      "  - 2509515_003_40kph.txt: [1757914226.6105194, 1757915737.2484002]\n",
      "  - 2509515_004_40kph.txt: [1757914226.6105194, 1757915832.3079796]\n",
      "  - 2509515_005_40kph.txt: [1757914226.6105194, 1757915938.87873]\n",
      "  - 2509515_001_40kph+vir.txt: [1757917557.797726, 1757918619.5116527]\n",
      "  - 2509515_002_40kph+vir.txt: [1757917557.797726, 1757918733.4559057]\n",
      "  - 2509515_003_40kph+vir.txt: [1757917557.797726, 1757918833.7639494]\n",
      "  - 2509515_004_40kph+vir.txt: [1757917557.797726, 1757918923.8949573]\n",
      "  - 2509515_005_40kph+vir.txt: [1757917557.797726, 1757919030.9990425]\n",
      "50kph:\n",
      "  - 2509515_001_50kph.txt: [1757914226.6105194, 1757916031.8637962]\n",
      "  - 2509515_001_50kph+vir.txt: [1757917557.797726, 1757919112.457446]\n",
      "  - 2509515_002_50kph+vir.txt: [1757917557.797726, 1757919220.2374144]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "FOLDER = Path(\"Tucsan_Sent\")         \n",
    "OUTPUT_DIR = FOLDER / \"_parsed\"     \n",
    "SPLIT_POSITION = True             \n",
    "SAVE_CSVS = False              \n",
    "SAVE_JSON = False                    \n",
    "\n",
    "LINE_RE = re.compile(\n",
    "    r\"^(?P<log_time>\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2},\\d{3})\\s+-\\s+INFO\\s+-\\s+RabbitMQ\\s+\\|\\s+Published message:\\s+(?P<payload>\\{.*\\})\\s*$\"\n",
    ")\n",
    "\n",
    "PREFERRED_ORDER = [\n",
    "    \"_log_time\",\n",
    "    \"timeStamp\", \"id\", \"cType\",\n",
    "    \"position\", \"pos_x\", \"pos_y\", \"pos_z\",\n",
    "    \"heading\", \"speed\",\n",
    "    \"width\", \"lengthf\", \"lengthb\", \"height\", \"accMax\",\n",
    "    \"t_veh_sent\", \"t_meta_rev\", \"t_meta_sent\",\n",
    "    \"t_intel_rev\", \"t_intel_sent\",\n",
    "    \"t_veh_rev\",\n",
    "]\n",
    "\n",
    "def parse_file(file_path: Path, split_position: bool = True) -> pd.DataFrame:\n",
    "    records = []\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.rstrip(\"\\n\")\n",
    "            m = LINE_RE.match(line.strip())\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            payload_str = m.group(\"payload\")\n",
    "            try:\n",
    "                payload = ast.literal_eval(payload_str)\n",
    "            except Exception:\n",
    "                last = payload_str.rfind(\"}\")\n",
    "                if last != -1:\n",
    "                    try:\n",
    "                        payload = ast.literal_eval(payload_str[: last + 1])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            payload[\"_log_time\"] = m.group(\"log_time\")\n",
    "\n",
    "            if split_position and isinstance(payload.get(\"position\"), str):\n",
    "                try:\n",
    "                    x_str, y_str, z_str = [t.strip() for t in payload[\"position\"].split(\",\")]\n",
    "                    payload[\"pos_x\"] = float(x_str)\n",
    "                    payload[\"pos_y\"] = float(y_str)\n",
    "                    payload[\"pos_z\"] = float(z_str)\n",
    "                except Exception:\n",
    "                    pass \n",
    "\n",
    "            records.append(payload)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    if \"_log_time\" in df.columns:\n",
    "        df[\"_log_time\"] = pd.to_datetime(df[\"_log_time\"], format=\"%Y-%m-%d %H:%M:%S,%f\", errors=\"coerce\")\n",
    "\n",
    "    if not df.empty:\n",
    "        ordered = [c for c in PREFERRED_ORDER if c in df.columns]\n",
    "        remaining = [c for c in df.columns if c not in ordered]\n",
    "        df = df[ordered + remaining]\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_folder(folder_path: Path, split_position: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    results: Dict[str, pd.DataFrame] = {}\n",
    "    for fp in sorted(folder_path.glob(\"*.txt\")):\n",
    "        df = parse_file(fp, split_position=split_position)\n",
    "        results[fp.name] = df\n",
    "    return results\n",
    "\n",
    "def get_kph_key(file_name: str) -> str:\n",
    "    m = re.search(r\"(\\d{1,3})\\s*kph\", file_name, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return f\"{int(m.group(1))}kph\"\n",
    "    m2 = re.search(r\"(\\d{1,3})\\s*kmh\", file_name, flags=re.IGNORECASE)\n",
    "    if m2:\n",
    "        return f\"{int(m2.group(1))}kph\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_run_window(df: pd.DataFrame) -> Tuple[float, float]:\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Empty DataFrame; no timeStamp values present.\")\n",
    "    col = \"timeStamp\" if \"timeStamp\" in df.columns else None\n",
    "    if col is None:\n",
    "        raise ValueError(\"Column 'timeStamp' not found in DataFrame.\")\n",
    "    s = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        raise ValueError(\"No valid numeric 'timeStamp' values.\")\n",
    "    # Align back to DataFrame order by sorting on timeStamp\n",
    "    df2 = df.loc[s.index].copy()\n",
    "    df2 = df2.sort_values(by=[col], kind=\"mergesort\")  # stable sort\n",
    "    return float(df2[col].iloc[0]), float(df2[col].iloc[-1])\n",
    "\n",
    "def save_dataframes_as_csv(dfs_by_file: Dict[str, pd.DataFrame], out_dir: Path) -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname, df in dfs_by_file.items():\n",
    "        stem = Path(fname).stem\n",
    "        out_path = out_dir / f\"{stem}.csv\"\n",
    "        # UTF-8 with BOM helps Excel show Unicode correctly\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "if not FOLDER.exists():\n",
    "    raise FileNotFoundError(f\"Folder not found: {FOLDER.resolve()}\")\n",
    "\n",
    "dfs = parse_folder(FOLDER, split_position=SPLIT_POSITION)\n",
    "\n",
    "if SAVE_CSVS:\n",
    "    save_dataframes_as_csv(dfs, OUTPUT_DIR)\n",
    "\n",
    "by_kph: Dict[str, List[List[float]]] = defaultdict(list)\n",
    "by_kph_with_files: Dict[str, List[List[object]]] = defaultdict(list)\n",
    "\n",
    "for fname, df in dfs.items():\n",
    "    if df is None or df.empty:\n",
    "        continue\n",
    "    try:\n",
    "        start_ts, end_ts = extract_run_window(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping {fname}: {e}\")\n",
    "        continue\n",
    "\n",
    "    key = get_kph_key(fname)  \n",
    "    by_kph[key].append([start_ts, end_ts])\n",
    "    by_kph_with_files[key].append([fname, start_ts, end_ts])\n",
    "\n",
    "for k in list(by_kph.keys()):\n",
    "    by_kph[k].sort(key=lambda x: x[0])\n",
    "for k in list(by_kph_with_files.keys()):\n",
    "    by_kph_with_files[k].sort(key=lambda x: x[1])\n",
    "\n",
    "print(\"=== Run windows by kph (lists of [start, end]) ===\")\n",
    "for k in sorted(by_kph.keys(), key=lambda x: (x==\"unknown\", int(x.replace(\"kph\",\"\")) if x!=\"unknown\" else 0)):\n",
    "    print(f\"{k} = {by_kph[k]}\")\n",
    "\n",
    "print(\"\\n=== Run windows by kph (with filenames) ===\")\n",
    "for k in sorted(by_kph_with_files.keys(), key=lambda x: (x==\"unknown\", int(x.replace(\"kph\",\"\")) if x!=\"unknown\" else 0)):\n",
    "    print(f\"{k}:\")\n",
    "    for fname, s, e in by_kph_with_files[k]:\n",
    "        print(f\"  - {fname}: [{s}, {e}]\")\n",
    "\n",
    "if SAVE_JSON:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with (OUTPUT_DIR / \"run_windows_by_kph.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(by_kph, f, ensure_ascii=False, indent=2)\n",
    "    with (OUTPUT_DIR / \"run_windows_by_kph_with_files.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(by_kph_with_files, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad72372-b5e6-4988-bf9b-d5440a5201fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_avante_sent: 326140 rows, columns = ['_ts_datetime', 'timeStamp', 'cType', 'id', 'position', 'pos_x', 'pos_y', 'pos_z', 'heading', 'speed']...\n",
      "df_sedan_to_vehicle: 108263 rows, columns = ['_ts_datetime', 'timeStamp', 'cType', 'id', 'position', 'pos_x', 'pos_y', 'pos_z', 'heading', 'speed']...\n",
      "df_tucsan_received: 14556 rows, columns = ['_ts_datetime', 'timeStamp', 'cType', 'id', 'position', 'pos_x', 'pos_y', 'pos_z', 'heading', 'speed']...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_jsonl_to_df(path: Path, split_position: bool = True) -> pd.DataFrame:\n",
    "    records = []\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path.resolve()}\")\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            start = line.find(\"{\")\n",
    "            end = line.rfind(\"}\")\n",
    "            if start == -1 or end == -1 or end <= start:\n",
    "                continue\n",
    "            payload_str = line[start:end+1]\n",
    "            try:\n",
    "                obj = json.loads(payload_str)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if split_position and isinstance(obj.get(\"position\"), str):\n",
    "                try:\n",
    "                    x_str, y_str, z_str = [t.strip() for t in obj[\"position\"].split(\",\")]\n",
    "                    obj[\"pos_x\"] = float(x_str)\n",
    "                    obj[\"pos_y\"] = float(y_str)\n",
    "                    obj[\"pos_z\"] = float(z_str)\n",
    "                except Exception:\n",
    "                    pass  \n",
    "\n",
    "            if \"timeStamp\" in obj:\n",
    "                try:\n",
    "                    obj[\"_ts_datetime\"] = pd.to_datetime(float(obj[\"timeStamp\"]), unit=\"s\", errors=\"coerce\")\n",
    "                except Exception:\n",
    "                    obj[\"_ts_datetime\"] = pd.NaT\n",
    "\n",
    "            records.append(obj)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    preferred = [\n",
    "        \"_ts_datetime\", \"timeStamp\", \"cType\", \"id\",\n",
    "        \"position\", \"pos_x\", \"pos_y\", \"pos_z\",\n",
    "        \"heading\", \"speed\", \"width\", \"lengthf\", \"lengthb\", \"height\", \"accMax\",\n",
    "        \"t_veh_sent\", \"t_meta_rev\", \"t_vir_obj_sent\", \"t_vir_obj_rev\",\n",
    "        \"t_meta_sent\", \"t_intel_rev\", \"t_intel_sent\", \"t_veh_rev\"\n",
    "    ]\n",
    "    if not df.empty:\n",
    "        ordered = [c for c in preferred if c in df.columns]\n",
    "        remaining = [c for c in df.columns if c not in ordered]\n",
    "        df = df[ordered + remaining]\n",
    "\n",
    "    return df\n",
    "\n",
    "base = Path(\".\")  \n",
    "\n",
    "files = {\n",
    "    \"df_avante_sent\": base / \"0915_Avante_Sent.txt\",\n",
    "    \"df_sedan_to_vehicle\": base / \"0915_sedan_to_vehicle.txt\",\n",
    "    \"df_tucsan_received\": base / \"0915_Tucsan_Received.txt\",\n",
    "}\n",
    "\n",
    "for var_name, path in files.items():\n",
    "    globals()[var_name] = load_jsonl_to_df(path, split_position=True)\n",
    "\n",
    "# Quick sanity prints\n",
    "for var_name in files.keys():\n",
    "    df = globals()[var_name]\n",
    "    print(f\"{var_name}: {len(df)} rows, columns = {list(df.columns)[:10]}{'...' if df.shape[1] > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b033362-ecbe-4d24-a924-8741b3f909b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 17 files from C:\\Users\\user\\Desktop\\mOS\\mOS_Data_Analysis\\Packet Loss\\Virtual_Received_By_Tucsan\n",
      " - speed_sueggest_250915_001_20kph+vir.txt: kept 820 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_001_30kph+vir.txt: kept 541 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_001_40kph+vir.txt: kept 431 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_001_50kph+vir.txt: kept 367 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_002_20kph+vir.txt: kept 757 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_002_30kph+vir.txt: kept 597 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_002_40kph+vir.txt: kept 427 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_002_50kph+vir.txt: kept 364 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_003_20kph+vir.txt: kept 704 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_003_30kph+vir.txt: kept 856 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_003_40kph+vir.txt: kept 433 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_004_20kph+vir.txt: kept 754 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_004_30kph+vir.txt: kept 542 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_004_40kph+vir.txt: kept 413 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_005_20kph+vir.txt: kept 740 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_005_30kph+vir.txt: kept 540 lines (traffic_id=3)\n",
      " - speed_sueggest_250915_005_40kph+vir.txt: kept 486 lines (traffic_id=3)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "VRT_FOLDER = Path(\"Virtual_Received_By_Tucsan\")  # folder next to the notebook\n",
    "TRAFFIC_ID_KEEP = 3\n",
    "\n",
    "_VRT_LINE_RE = re.compile(\n",
    "    r\"\"\"^\\[INFO\\]\\s+\\[(?P<ros_time>\\d+(?:\\.\\d+)?)\\]:\\s*\n",
    "        traffic_id:\\s*(?P<traffic_id>\\d+),\\s*\n",
    "        t_veh_sent:\\s*(?P<t_veh_sent>-?\\d+(?:\\.\\d+)?),\\s*\n",
    "        current:\\s*(?P<current>-?\\d+(?:\\.\\d+)?),\\s*\n",
    "        latency:\\s*(?P<latency>-?\\d+(?:\\.\\d+)?)\n",
    "        \\s*$\"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "\n",
    "def _kph_from_name(name: str) -> str:\n",
    "    m = re.search(r\"(\\d{1,3})\\s*(kph|kmh)\", name, flags=re.IGNORECASE)\n",
    "    return f\"{int(m.group(1))}kph\" if m else \"unknown\"\n",
    "\n",
    "def parse_virtual_received_file(path: Path, traffic_id_keep: int = 3) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line or \"speed_suggest\" in line:  # explicitly ignore suggestion lines\n",
    "                continue\n",
    "            m = _VRT_LINE_RE.match(line)\n",
    "            if not m:\n",
    "                continue\n",
    "            gd = m.groupdict()\n",
    "            if int(gd[\"traffic_id\"]) != traffic_id_keep:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"source_file\": path.name,\n",
    "                \"kph\": _kph_from_name(path.name),\n",
    "                \"_ros_time\": float(gd[\"ros_time\"]),\n",
    "                \"traffic_id\": int(gd[\"traffic_id\"]),\n",
    "                \"t_veh_sent\": float(gd[\"t_veh_sent\"]),\n",
    "                \"current\": float(gd[\"current\"]),\n",
    "                \"latency\": float(gd[\"latency\"]),\n",
    "            })\n",
    "    df = pd.DataFrame.from_records(rows)\n",
    "    if not df.empty:\n",
    "        df[\"_ros_datetime\"] = pd.to_datetime(df[\"_ros_time\"], unit=\"s\", errors=\"coerce\")\n",
    "        preferred = [\"source_file\", \"kph\", \"_ros_datetime\", \"_ros_time\",\n",
    "                     \"traffic_id\", \"t_veh_sent\", \"current\", \"latency\"]\n",
    "        ordered = [c for c in preferred if c in df.columns]\n",
    "        remain = [c for c in df.columns if c not in ordered]\n",
    "        df = df[ordered + remain].sort_values(\"_ros_time\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def parse_virtual_received_folder(folder: Path, traffic_id_keep: int = 3) -> Dict[str, pd.DataFrame]:\n",
    "    results: Dict[str, pd.DataFrame] = {}\n",
    "    for fp in sorted(folder.glob(\"*.txt\")):\n",
    "        results[fp.name] = parse_virtual_received_file(fp, traffic_id_keep=traffic_id_keep)\n",
    "    return results\n",
    "\n",
    "if not VRT_FOLDER.exists():\n",
    "    raise FileNotFoundError(f\"Folder not found: {VRT_FOLDER.resolve()}\")\n",
    "\n",
    "vrt_dfs_by_file = parse_virtual_received_folder(VRT_FOLDER, traffic_id_keep=TRAFFIC_ID_KEEP)\n",
    "\n",
    "print(f\"Parsed {len(vrt_dfs_by_file)} files from {VRT_FOLDER.resolve()}\")\n",
    "for name, df in vrt_dfs_by_file.items():\n",
    "    print(f\" - {name}: kept {len(df)} lines (traffic_id={TRAFFIC_ID_KEEP})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64275314-cea1-4817-8e89-77acfbc9e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metaverse Server -> CAV packet loss per receiver file ===\n",
      "                              receiver_file  speed  start_t_veh_sent  \\\n",
      "0   speed_sueggest_250915_001_20kph+vir.txt  20kph 1757917458.993619   \n",
      "1   speed_sueggest_250915_001_30kph+vir.txt  30kph 1757918013.503961   \n",
      "2   speed_sueggest_250915_001_40kph+vir.txt  40kph 1757918531.066991   \n",
      "3   speed_sueggest_250915_001_50kph+vir.txt  50kph 1757919030.999043   \n",
      "4   speed_sueggest_250915_002_20kph+vir.txt  20kph 1757917562.989060   \n",
      "5   speed_sueggest_250915_002_30kph+vir.txt  30kph 1757918125.537244   \n",
      "6   speed_sueggest_250915_002_40kph+vir.txt  40kph 1757918619.511653   \n",
      "7   speed_sueggest_250915_002_50kph+vir.txt  50kph 1757919112.457446   \n",
      "8   speed_sueggest_250915_003_20kph+vir.txt  20kph 1757917675.744356   \n",
      "9   speed_sueggest_250915_003_30kph+vir.txt  30kph 1757918237.957331   \n",
      "10  speed_sueggest_250915_003_40kph+vir.txt  40kph 1757918733.455906   \n",
      "11  speed_sueggest_250915_004_20kph+vir.txt  20kph 1757917787.603899   \n",
      "12  speed_sueggest_250915_004_30kph+vir.txt  30kph 1757918350.304189   \n",
      "13  speed_sueggest_250915_004_40kph+vir.txt  40kph 1757918833.663914   \n",
      "14  speed_sueggest_250915_005_20kph+vir.txt  20kph 1757917900.718157   \n",
      "15  speed_sueggest_250915_005_30kph+vir.txt  30kph 1757918432.884026   \n",
      "16  speed_sueggest_250915_005_40kph+vir.txt  40kph 1757918923.894957   \n",
      "\n",
      "      end_t_veh_sent  sender_count  receiver_count  matched_count  \\\n",
      "0  1757917562.818997           319             319            319   \n",
      "1  1757918121.666747           206             206            206   \n",
      "2  1757918617.551624           160             160            160   \n",
      "3  1757919110.007601           129             129            129   \n",
      "4  1757917675.454439           286             286            286   \n",
      "5  1757918235.767260           248             248            248   \n",
      "6  1757918730.095930           162             162            162   \n",
      "7  1757919218.007501           158             158            158   \n",
      "8  1757917785.783786           290             290            290   \n",
      "9  1757918349.390719           358             358            358   \n",
      "10 1757918833.003974           187             187            187   \n",
      "11 1757917898.328140           290             290            290   \n",
      "12 1757918431.024091           222             222            222   \n",
      "13 1757918921.464790           167             167            167   \n",
      "14 1757918012.393963           287             287            287   \n",
      "15 1757918529.326908           220             220            220   \n",
      "16 1757919030.159124           210             210            210   \n",
      "\n",
      "    packet_loss_pct  tol_sec  \n",
      "0          0.000000 0.000001  \n",
      "1          0.000000 0.000001  \n",
      "2          0.000000 0.000001  \n",
      "3          0.000000 0.000001  \n",
      "4          0.000000 0.000001  \n",
      "5          0.000000 0.000001  \n",
      "6          0.000000 0.000001  \n",
      "7          0.000000 0.000001  \n",
      "8          0.000000 0.000001  \n",
      "9          0.000000 0.000001  \n",
      "10         0.000000 0.000001  \n",
      "11         0.000000 0.000001  \n",
      "12         0.000000 0.000001  \n",
      "13         0.000000 0.000001  \n",
      "14         0.000000 0.000001  \n",
      "15         0.000000 0.000001  \n",
      "16         0.000000 0.000001  \n",
      "\n",
      "=== Packet loss summary by speed (Server -> CAV) ===\n",
      "   speed     mean      std      min       q1       q3      max\n",
      "0  20kph 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "1  30kph 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "2  40kph 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "3  50kph 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "TOL_SEC = 1e-6  \n",
    "\n",
    "if \"df_sedan_to_vehicle\" not in globals():\n",
    "    raise NameError(\"df_sedan_to_vehicle not found. Load 0915_sedan_to_vehicle.txt first.\")\n",
    "if \"vrt_dfs_by_file\" not in globals():\n",
    "    raise NameError(\"vrt_dfs_by_file not found. Parse Virtual_Received_By_Tucsan first.\")\n",
    "\n",
    "if \"t_veh_sent\" not in df_sedan_to_vehicle.columns:\n",
    "    raise KeyError(\"df_sedan_to_vehicle missing 't_veh_sent' column.\")\n",
    "df_sedan_to_vehicle[\"t_veh_sent\"] = pd.to_numeric(df_sedan_to_vehicle[\"t_veh_sent\"], errors=\"coerce\")\n",
    "\n",
    "for _name, _df in vrt_dfs_by_file.items():\n",
    "    if not _df.empty and \"t_veh_sent\" not in _df.columns:\n",
    "        raise KeyError(f\"Receiver df '{_name}' missing 't_veh_sent' column.\")\n",
    "\n",
    "def _kph_from_name(name: str) -> str:\n",
    "    m = re.search(r\"(\\d{1,3})\\s*(kph|kmh)\", name, flags=re.IGNORECASE)\n",
    "    return f\"{int(m.group(1))}kph\" if m else \"unknown\"\n",
    "\n",
    "def _speed_sort_key(k: str):\n",
    "    return (k == \"unknown\",\n",
    "            int(k.replace(\"kph\", \"\")) if isinstance(k, str) and k.endswith(\"kph\") and k[:-3].isdigit() else math.inf)\n",
    "\n",
    "def _unique_sorted_positive(series: pd.Series) -> np.ndarray:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna().astype(float)\n",
    "    s = s[s > 0]\n",
    "    return np.unique(np.sort(s.values)) if not s.empty else np.array([], dtype=float)\n",
    "\n",
    "def _greedy_match_count(a: np.ndarray, b: np.ndarray, tol: float) -> int:\n",
    "    i = j = 0\n",
    "    matched = 0\n",
    "    na, nb = len(a), len(b)\n",
    "    while i < na and j < nb:\n",
    "        diff = a[i] - b[j]\n",
    "        if abs(diff) <= tol:\n",
    "            matched += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif diff < 0:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return matched\n",
    "\n",
    "rows_srv2cav = []\n",
    "\n",
    "for recv_name, recv_df in sorted(vrt_dfs_by_file.items()):\n",
    "    recv_vals = _unique_sorted_positive(recv_df.get(\"t_veh_sent\", pd.Series(dtype=float)))\n",
    "    if recv_vals.size == 0:\n",
    "        rows_srv2cav.append({\n",
    "            \"receiver_file\": recv_name,\n",
    "            \"speed\": _kph_from_name(recv_name),\n",
    "            \"start_t_veh_sent\": np.nan,\n",
    "            \"end_t_veh_sent\": np.nan,\n",
    "            \"sender_count\": np.nan,\n",
    "            \"receiver_count\": 0,\n",
    "            \"matched_count\": 0,\n",
    "            \"packet_loss_pct\": np.nan,\n",
    "            \"tol_sec\": TOL_SEC,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    start_t = float(recv_vals[0])\n",
    "    end_t   = float(recv_vals[-1])\n",
    "\n",
    "    sender_vals = _unique_sorted_positive(\n",
    "        df_sedan_to_vehicle.loc[\n",
    "            (df_sedan_to_vehicle[\"t_veh_sent\"] >= start_t) &\n",
    "            (df_sedan_to_vehicle[\"t_veh_sent\"] <= end_t),\n",
    "            \"t_veh_sent\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    sender_count = int(sender_vals.size)\n",
    "    receiver_count = int(recv_vals.size)\n",
    "    matched = _greedy_match_count(sender_vals, recv_vals, tol=TOL_SEC)\n",
    "\n",
    "    pkt_loss_pct = (100.0 * (1.0 - matched / sender_count)) if sender_count > 0 else np.nan\n",
    "\n",
    "    rows_srv2cav.append({\n",
    "        \"receiver_file\": recv_name,\n",
    "        \"speed\": _kph_from_name(recv_name),\n",
    "        \"start_t_veh_sent\": start_t,\n",
    "        \"end_t_veh_sent\": end_t,\n",
    "        \"sender_count\": sender_count,\n",
    "        \"receiver_count\": receiver_count,\n",
    "        \"matched_count\": matched,\n",
    "        \"packet_loss_pct\": pkt_loss_pct,\n",
    "        \"tol_sec\": TOL_SEC,\n",
    "    })\n",
    "\n",
    "pktloss_server_to_cav_runs = pd.DataFrame(rows_srv2cav)\n",
    "\n",
    "def _q1(x): return x.quantile(0.25)\n",
    "def _q3(x): return x.quantile(0.75)\n",
    "\n",
    "pktloss_server_to_cav_by_speed = (\n",
    "    pktloss_server_to_cav_runs\n",
    "    .dropna(subset=[\"packet_loss_pct\"])\n",
    "    .groupby(\"speed\")[\"packet_loss_pct\"]\n",
    "    .agg(mean=\"mean\", std=\"std\", min=\"min\", q1=_q1, q3=_q3, max=\"max\")\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"speed\", key=lambda s: s.map(_speed_sort_key))\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:0.6f}\")\n",
    "print(\"=== Metaverse Server -> CAV packet loss per receiver file ===\")\n",
    "print(pktloss_server_to_cav_runs[[\n",
    "    \"receiver_file\",\"speed\",\"start_t_veh_sent\",\"end_t_veh_sent\",\n",
    "    \"sender_count\",\"receiver_count\",\"matched_count\",\"packet_loss_pct\",\"tol_sec\"\n",
    "]])\n",
    "\n",
    "print(\"\\n=== Packet loss summary by speed (Server -> CAV) ===\")\n",
    "print(pktloss_server_to_cav_by_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a87272-98a9-463c-b2db-3ac377b08067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              receiver_file  sender_in_recv_count  \\\n",
      "0   speed_sueggest_250915_001_20kph+vir.txt                   319   \n",
      "1   speed_sueggest_250915_001_30kph+vir.txt                   206   \n",
      "2   speed_sueggest_250915_001_40kph+vir.txt                   160   \n",
      "3   speed_sueggest_250915_001_50kph+vir.txt                   129   \n",
      "4   speed_sueggest_250915_002_20kph+vir.txt                   286   \n",
      "5   speed_sueggest_250915_002_30kph+vir.txt                   248   \n",
      "6   speed_sueggest_250915_002_40kph+vir.txt                   162   \n",
      "7   speed_sueggest_250915_002_50kph+vir.txt                   158   \n",
      "8   speed_sueggest_250915_003_20kph+vir.txt                   290   \n",
      "9   speed_sueggest_250915_003_30kph+vir.txt                   358   \n",
      "10  speed_sueggest_250915_003_40kph+vir.txt                   187   \n",
      "11  speed_sueggest_250915_004_20kph+vir.txt                   290   \n",
      "12  speed_sueggest_250915_004_30kph+vir.txt                   222   \n",
      "13  speed_sueggest_250915_004_40kph+vir.txt                   167   \n",
      "14  speed_sueggest_250915_005_20kph+vir.txt                   287   \n",
      "15  speed_sueggest_250915_005_30kph+vir.txt                   220   \n",
      "16  speed_sueggest_250915_005_40kph+vir.txt                   210   \n",
      "\n",
      "    receiver_count  matched_in_recv  loss_pct_in_recv  sender_in_exp_count  \\\n",
      "0              319              319          0.000000                  320   \n",
      "1              206              206          0.000000                  206   \n",
      "2              160              160          0.000000                  160   \n",
      "3              129              129          0.000000                  130   \n",
      "4              286              286          0.000000                  286   \n",
      "5              248              248          0.000000                  249   \n",
      "6              162              162          0.000000                  162   \n",
      "7              158              158          0.000000                  158   \n",
      "8              290              290          0.000000                  290   \n",
      "9              358              358          0.000000                  358   \n",
      "10             187              187          0.000000                  188   \n",
      "11             290              290          0.000000                  291   \n",
      "12             222              222          0.000000                  222   \n",
      "13             167              167          0.000000                  167   \n",
      "14             287              287          0.000000                  288   \n",
      "15             220              220          0.000000                  221   \n",
      "16             210              210          0.000000                  210   \n",
      "\n",
      "    matched_in_exp  loss_pct_in_exp  \n",
      "0              319         0.312500  \n",
      "1              206         0.000000  \n",
      "2              160         0.000000  \n",
      "3              129         0.769231  \n",
      "4              286         0.000000  \n",
      "5              248         0.401606  \n",
      "6              162         0.000000  \n",
      "7              158         0.000000  \n",
      "8              290         0.000000  \n",
      "9              358         0.000000  \n",
      "10             187         0.531915  \n",
      "11             290         0.343643  \n",
      "12             222         0.000000  \n",
      "13             167         0.000000  \n",
      "14             287         0.347222  \n",
      "15             220         0.452489  \n",
      "16             210         0.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EXPAND_SEC = 0.05       \n",
    "\n",
    "TOL_SEC = 1e-6       \n",
    "\n",
    "def _uniq_sorted_pos(x: pd.Series) -> np.ndarray:\n",
    "    s = pd.to_numeric(x, errors=\"coerce\").dropna().astype(float)\n",
    "    s = s[s > 0]\n",
    "    return np.unique(np.sort(s.values)) if not s.empty else np.array([], dtype=float)\n",
    "\n",
    "def _greedy_match(a: np.ndarray, b: np.ndarray, tol: float) -> int:\n",
    "    i = j = 0\n",
    "    m = 0\n",
    "    na, nb = len(a), len(b)\n",
    "    while i < na and j < nb:\n",
    "        d = a[i] - b[j]\n",
    "        if abs(d) <= tol:\n",
    "            m += 1; i += 1; j += 1\n",
    "        elif d < 0:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return m\n",
    "\n",
    "rows_check = []\n",
    "for recv_name, recv_df in sorted(vrt_dfs_by_file.items()):\n",
    "    recv_vals = _uniq_sorted_pos(recv_df.get(\"t_veh_sent\", pd.Series(dtype=float)))\n",
    "    if recv_vals.size == 0:\n",
    "        rows_check.append({\n",
    "            \"receiver_file\": recv_name,\n",
    "            \"sender_in_recv_count\": np.nan,\n",
    "            \"receiver_count\": 0,\n",
    "            \"matched_in_recv\": 0,\n",
    "            \"loss_pct_in_recv\": np.nan,\n",
    "            \"sender_in_exp_count\": np.nan,\n",
    "            \"matched_in_exp\": 0,\n",
    "            \"loss_pct_in_exp\": np.nan,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    r0, r1 = float(recv_vals[0]), float(recv_vals[-1])\n",
    "\n",
    "    sender_vals_full = _uniq_sorted_pos(df_sedan_to_vehicle[\"t_veh_sent\"])\n",
    "    sender_in_recv = sender_vals_full[(sender_vals_full >= r0) & (sender_vals_full <= r1)]\n",
    "    sender_in_exp  = sender_vals_full[(sender_vals_full >= r0 - 0.09) & (sender_vals_full <= r1 + 0.05)]\n",
    "    \n",
    "    matched_in_recv = _greedy_match(sender_in_recv, recv_vals, TOL_SEC)\n",
    "    loss_pct_in_recv = 100.0 * (1.0 - matched_in_recv / len(sender_in_recv)) if len(sender_in_recv) > 0 else np.nan\n",
    "\n",
    "    matched_in_exp = _greedy_match(sender_in_exp, recv_vals, TOL_SEC)\n",
    "    loss_pct_in_exp = 100.0 * (1.0 - matched_in_exp / len(sender_in_exp)) if len(sender_in_exp) > 0 else np.nan\n",
    "\n",
    "    rows_check.append({\n",
    "        \"receiver_file\": recv_name,\n",
    "        \"sender_in_recv_count\": int(len(sender_in_recv)),\n",
    "        \"receiver_count\": int(len(recv_vals)),\n",
    "        \"matched_in_recv\": int(matched_in_recv),\n",
    "        \"loss_pct_in_recv\": loss_pct_in_recv,\n",
    "        \"sender_in_exp_count\": int(len(sender_in_exp)),\n",
    "        \"matched_in_exp\": int(matched_in_exp),\n",
    "        \"loss_pct_in_exp\": loss_pct_in_exp,\n",
    "    })\n",
    "\n",
    "srv2cav_window_check = pd.DataFrame(rows_check)\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:0.6f}\")\n",
    "print(srv2cav_window_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6182892f-e088-482a-8933-3a67626ffd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Per-speed summary (Receiver window) ===\n",
      "   speed  n_files  sender_in_recv_mean  receiver_count_mean  loss_recv_mean  \\\n",
      "0  20kph        5           294.400000           294.400000        0.000000   \n",
      "1  30kph        5           250.800000           250.800000        0.000000   \n",
      "2  40kph        5           177.200000           177.200000        0.000000   \n",
      "3  50kph        2           143.500000           143.500000        0.000000   \n",
      "\n",
      "   loss_recv_std  loss_recv_min  loss_recv_q1  loss_recv_median  loss_recv_q3  \\\n",
      "0       0.000000       0.000000      0.000000          0.000000      0.000000   \n",
      "1       0.000000       0.000000      0.000000          0.000000      0.000000   \n",
      "2       0.000000       0.000000      0.000000          0.000000      0.000000   \n",
      "3       0.000000       0.000000      0.000000          0.000000      0.000000   \n",
      "\n",
      "   loss_recv_max  \n",
      "0       0.000000  \n",
      "1       0.000000  \n",
      "2       0.000000  \n",
      "3       0.000000  \n",
      "\n",
      "=== Per-speed summary (Expanded window) ===\n",
      "   speed  n_files  sender_in_exp_mean  loss_exp_mean  loss_exp_std  \\\n",
      "0  20kph        5          295.000000       0.200673      0.183686   \n",
      "1  30kph        5          251.200000       0.170819      0.234594   \n",
      "2  40kph        5          177.400000       0.106383      0.237880   \n",
      "3  50kph        2          144.000000       0.384615      0.543928   \n",
      "\n",
      "   loss_exp_min  loss_exp_q1  loss_exp_median  loss_exp_q3  loss_exp_max  \\\n",
      "0      0.000000     0.000000         0.312500     0.343643      0.347222   \n",
      "1      0.000000     0.000000         0.000000     0.401606      0.452489   \n",
      "2      0.000000     0.000000         0.000000     0.000000      0.531915   \n",
      "3      0.000000     0.192308         0.384615     0.576923      0.769231   \n",
      "\n",
      "   pct_runs_more_matched  pct_runs_less_matched  delta_loss_mean  \\\n",
      "0               0.000000               0.000000         0.200673   \n",
      "1               0.000000               0.000000         0.170819   \n",
      "2               0.000000               0.000000         0.106383   \n",
      "3               0.000000               0.000000         0.384615   \n",
      "\n",
      "   delta_loss_median  \n",
      "0           0.312500  \n",
      "1           0.000000  \n",
      "2           0.000000  \n",
      "3           0.384615  \n",
      "\n",
      "=== Overall loss distribution — Receiver window ===\n",
      "count   17.000000\n",
      "mean     0.000000\n",
      "std      0.000000\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      0.000000\n",
      "max      0.000000\n",
      "Name: loss_in_recv_desc, dtype: float64\n",
      "\n",
      "=== Overall loss distribution — Expanded window ===\n",
      "count   17.000000\n",
      "mean     0.185800\n",
      "std      0.248762\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      0.347222\n",
      "max      0.769231\n",
      "Name: loss_in_exp_desc, dtype: float64\n",
      "\n",
      "=== Quick diagnostics ===\n",
      "Files where expanded window increased matched count: 0 / 17 (0.00%)\n",
      "Files where expanded window decreased matched count: 0 / 17 (0.00%)\n",
      "Mean Δloss (expanded - receiver) across all files: 0.185800 %\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"srv2cav_window_check\" not in globals():\n",
    "    raise NameError(\"srv2cav_window_check not found. Run the window-check cell first.\")\n",
    "\n",
    "def _kph_from_name(name: str) -> str:\n",
    "    m = re.search(r\"(\\d{1,3})\\s*(kph|kmh)\", str(name), flags=re.IGNORECASE)\n",
    "    return f\"{int(m.group(1))}kph\" if m else \"unknown\"\n",
    "\n",
    "def _speed_sort_key(k: str):\n",
    "    try:\n",
    "        return (k == \"unknown\", int(k.replace(\"kph\", \"\")))\n",
    "    except Exception:\n",
    "        return (True, 10**9)\n",
    "\n",
    "def _q1(x): return x.quantile(0.25)\n",
    "def _q3(x): return x.quantile(0.75)\n",
    "\n",
    "df_sum = srv2cav_window_check.copy()\n",
    "\n",
    "df_sum[\"speed\"] = df_sum[\"receiver_file\"].map(_kph_from_name)\n",
    "df_sum[\"retention_in_recv\"] = 100.0 - df_sum[\"loss_pct_in_recv\"]\n",
    "df_sum[\"retention_in_exp\"]  = 100.0 - df_sum[\"loss_pct_in_exp\"]\n",
    "df_sum[\"delta_loss_pct\"]    = df_sum[\"loss_pct_in_exp\"] - df_sum[\"loss_pct_in_recv\"]\n",
    "df_sum[\"delta_matched\"]     = df_sum[\"matched_in_exp\"] - df_sum[\"matched_in_recv\"]\n",
    "df_sum[\"delta_sender\"]      = df_sum[\"sender_in_exp_count\"] - df_sum[\"sender_in_recv_count\"]\n",
    "\n",
    "summary_recv = (\n",
    "    df_sum\n",
    "    .groupby(\"speed\", dropna=False)\n",
    "    .agg(\n",
    "        n_files=(\"receiver_file\", \"count\"),\n",
    "        sender_in_recv_mean=(\"sender_in_recv_count\", \"mean\"),\n",
    "        receiver_count_mean=(\"receiver_count\", \"mean\"),\n",
    "        loss_recv_mean=(\"loss_pct_in_recv\", \"mean\"),\n",
    "        loss_recv_std=(\"loss_pct_in_recv\", \"std\"),\n",
    "        loss_recv_min=(\"loss_pct_in_recv\", \"min\"),\n",
    "        loss_recv_q1=(\"loss_pct_in_recv\", _q1),\n",
    "        loss_recv_median=(\"loss_pct_in_recv\", \"median\"),\n",
    "        loss_recv_q3=(\"loss_pct_in_recv\", _q3),\n",
    "        loss_recv_max=(\"loss_pct_in_recv\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"speed\", key=lambda s: s.map(_speed_sort_key))\n",
    ")\n",
    "\n",
    "summary_exp = (\n",
    "    df_sum\n",
    "    .groupby(\"speed\", dropna=False)\n",
    "    .agg(\n",
    "        n_files=(\"receiver_file\", \"count\"),\n",
    "        sender_in_exp_mean=(\"sender_in_exp_count\", \"mean\"),\n",
    "        loss_exp_mean=(\"loss_pct_in_exp\", \"mean\"),\n",
    "        loss_exp_std=(\"loss_pct_in_exp\", \"std\"),\n",
    "        loss_exp_min=(\"loss_pct_in_exp\", \"min\"),\n",
    "        loss_exp_q1=(\"loss_pct_in_exp\", _q1),\n",
    "        loss_exp_median=(\"loss_pct_in_exp\", \"median\"),\n",
    "        loss_exp_q3=(\"loss_pct_in_exp\", _q3),\n",
    "        loss_exp_max=(\"loss_pct_in_exp\", \"max\"),\n",
    "        pct_runs_more_matched=(\"delta_matched\", lambda s: 100.0 * (s > 0).mean()),\n",
    "        pct_runs_less_matched=(\"delta_matched\", lambda s: 100.0 * (s < 0).mean()),\n",
    "        delta_loss_mean=(\"delta_loss_pct\", \"mean\"),\n",
    "        delta_loss_median=(\"delta_loss_pct\", \"median\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"speed\", key=lambda s: s.map(_speed_sort_key))\n",
    ")\n",
    "\n",
    "overall_recv = df_sum[\"loss_pct_in_recv\"].describe(percentiles=[0.25, 0.5, 0.75]).rename(\"loss_in_recv_desc\")\n",
    "overall_exp  = df_sum[\"loss_pct_in_exp\"].describe(percentiles=[0.25, 0.5, 0.75]).rename(\"loss_in_exp_desc\")\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:0.6f}\")\n",
    "\n",
    "print(\"=== Per-speed summary (Receiver window) ===\")\n",
    "print(summary_recv)\n",
    "\n",
    "print(\"\\n=== Per-speed summary (Expanded window) ===\")\n",
    "print(summary_exp)\n",
    "\n",
    "print(\"\\n=== Overall loss distribution — Receiver window ===\")\n",
    "print(overall_recv)\n",
    "\n",
    "print(\"\\n=== Overall loss distribution — Expanded window ===\")\n",
    "print(overall_exp)\n",
    "\n",
    "print(\"\\n=== Quick diagnostics ===\")\n",
    "print(f\"Files where expanded window increased matched count: {(df_sum['delta_matched'] > 0).sum()} / {len(df_sum)} \"\n",
    "      f\"({100.0*(df_sum['delta_matched']>0).mean():.2f}%)\")\n",
    "print(f\"Files where expanded window decreased matched count: {(df_sum['delta_matched'] < 0).sum()} / {len(df_sum)} \"\n",
    "      f\"({100.0*(df_sum['delta_matched']<0).mean():.2f}%)\")\n",
    "print(f\"Mean Δloss (expanded - receiver) across all files: {df_sum['delta_loss_pct'].mean():.6f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb85f61-dd36-479e-9747-d9860bbbc557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a55d76-aada-4e76-9c24-f7147310bcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4aa552-9298-4b06-aefd-5461d0da3cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
